{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustav2k22/GRP12_BCS206_StudentDepression/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgjR2sl0nq04"
      },
      "outputs": [],
      "source": [
        "# Adding all needed libraries\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn plotly\n",
        "\n",
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, log_loss, roc_auc_score,\n",
        "                            precision_score, recall_score, f1_score,\n",
        "                            confusion_matrix, classification_report, roc_curve)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setting style for better visual plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8IpkW56dyI-",
        "outputId": "18e3bd0b-c9a7-4ce8-8f71-402bf9cbd047"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('//content/drive/MyDrive/GRP12_BCS206_StudentDepression/dataset/student_depression_dataset.csv')\n",
        "\n",
        "# Basic dataset information\n",
        "print(\"📊 DATASET OVERVIEW\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(\"\\n📋 First 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n📈 Dataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n📊 Statistical Summary:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n🔍 Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Based on your screenshot, the target column appears to be 'Depression'\n",
        "target_column = 'Depression'\n",
        "print(f\"\\n🎯 Target Column: {target_column}\")\n",
        "print(f\"Target Distribution:\")\n",
        "if target_column in df.columns:\n",
        "    print(df[target_column].value_counts())\n",
        "else:\n",
        "    print(\"⚠️ Please check the exact target column name\")"
      ],
      "metadata": {
        "id": "0BefHtSstmD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🎯 DATASET JUSTIFICATION\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "📌 DATASET: Student Depression Dataset\n",
        "\n",
        "🔍 RELEVANCE TO PREDICTIVE TASK:\n",
        "- Mental health is a critical issue among students globally\n",
        "- Early detection of depression can enable timely intervention\n",
        "- Dataset contains comprehensive student information including:\n",
        "  * Demographics: Age, Gender, City\n",
        "  * Academic factors: Academic Pressure, CGPA, Study Satisfaction\n",
        "  * Social factors: Work Pressure, Job Satisfaction\n",
        "  * Health factors: Sleep Duration, Dietary Habits\n",
        "  * Mental health indicators: Suicidal thoughts, Family History\n",
        "- Classification problem: Predict presence/absence of depression\n",
        "- Real-world application with significant social impact\n",
        "\n",
        "🎯 PREDICTIVE OBJECTIVE:\n",
        "Build an advanced ensemble model to classify students as having depression based on:\n",
        "- Academic performance and pressure levels\n",
        "- Social and work-related stress factors\n",
        "- Lifestyle and health indicators\n",
        "- Demographics and family history\n",
        "- Sleep patterns and dietary habits\n",
        "\"\"\")\n",
        "\n",
        "# Analyze target variable using your dataset structure\n",
        "target_col = 'Depression'  # Based on your screenshot\n",
        "if target_col in df.columns:\n",
        "    print(f\"\\n📊 Target Variable Distribution:\")\n",
        "    print(df[target_col].value_counts())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Create subplot for better visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Count plot\n",
        "    df[target_col].value_counts().plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral'])\n",
        "    ax1.set_title('Distribution of Depression Cases')\n",
        "    ax1.set_xlabel('Depression Status')\n",
        "    ax1.set_ylabel('Count')\n",
        "    ax1.tick_params(axis='x', rotation=0)\n",
        "\n",
        "    # Pie chart\n",
        "    df[target_col].value_counts().plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=['skyblue', 'lightcoral'])\n",
        "    ax2.set_title('Depression Cases Percentage')\n",
        "    ax2.set_ylabel('')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️ Please identify the correct target column name in your dataset\")"
      ],
      "metadata": {
        "id": "ESJzoePVullg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🔧 DATA PREPROCESSING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Handling the preprocessing properly\n",
        "print(\"Original dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# Target column\n",
        "target_col = 'Depression'\n",
        "\n",
        "# Handle missing values if any\n",
        "print(\"Checking for missing values...\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Fill missing values if they exist\n",
        "if missing_values.sum() > 0:\n",
        "    # Fill numerical columns with median\n",
        "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n",
        "\n",
        "    # Fill categorical columns with mode\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        if col != target_col:\n",
        "            df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "# Encode categorical variables\n",
        "print(\"Encoding categorical variables...\")\n",
        "label_encoders = {}\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in categorical_columns:\n",
        "    if col != target_col:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "        print(f\"Encoded {col}: {le.classes_}\")\n",
        "\n",
        "# Prepare features and target\n",
        "X = df.drop([target_col], axis=1)\n",
        "y = df[target_col]\n",
        "\n",
        "# Encode target variable if it's categorical\n",
        "if y.dtype == 'object':\n",
        "    le_target = LabelEncoder()\n",
        "    y = le_target.fit_transform(y)\n",
        "    print(f\"Target classes: {le_target.classes_}\")\n",
        "\n",
        "print(f\"\\n✅ Features shape: {X.shape}\")\n",
        "print(f\"✅ Target shape: {y.shape}\")\n",
        "print(f\"✅ Feature columns: {X.columns.tolist()}\")\n",
        "print(f\"✅ Target distribution: {np.bincount(y)}\")"
      ],
      "metadata": {
        "id": "YGecv0DRu2fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🤖 ENSEMBLE LEARNING/ ADVANCED STACKING MODEL DEVELOPMENT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Import additional libraries for stacking\n",
        "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"✅ Training set: {X_train_scaled.shape}\")\n",
        "print(f\"✅ Testing set: {X_test_scaled.shape}\")\n",
        "\n",
        "# Define base models for stacking\n",
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(random_state=42)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
        "    ('nb', GaussianNB())\n",
        "]\n",
        "\n",
        "# Define meta-model (final estimator)\n",
        "meta_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Create stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,  # 5-fold cross-validation for generating meta-features\n",
        "    stack_method='predict_proba',  # Use probabilities for meta-features\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"🚀 Training Stacking Classifier...\")\n",
        "print(\"Base Models:\")\n",
        "for name, model in base_models:\n",
        "    print(f\"  • {name}: {model.__class__.__name__}\")\n",
        "print(f\"Meta Model: {meta_model.__class__.__name__}\")\n",
        "\n",
        "# Train the Ensemble/ stacking classifier\n",
        "stacking_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\n✅ Ensemble/ Stacking Classifier trained successfully!\")\n",
        "\n",
        "# Also train individual base models for comparison\n",
        "print(\"\\n📊 INDIVIDUAL BASE MODEL PERFORMANCE:\")\n",
        "individual_results = {}\n",
        "\n",
        "for name, model in base_models:\n",
        "    # Train individual model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    individual_results[name] = {'accuracy': accuracy, 'auc': auc, 'f1': f1}\n",
        "    print(f\"{name.upper():>3}: Accuracy={accuracy:.4f}, AUC={auc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "# Test stacking classifier\n",
        "print(f\"\\n🏆 ENSEMBLE/ STACKING CLASSIFIER PERFORMANCE:\")\n",
        "stacking_pred = stacking_clf.predict(X_test_scaled)\n",
        "stacking_pred_proba = stacking_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
        "stacking_auc = roc_auc_score(y_test, stacking_pred_proba)\n",
        "stacking_f1 = f1_score(y_test, stacking_pred)\n",
        "\n",
        "print(f\"ENSEMBLE/ STACKING: Accuracy={stacking_accuracy:.4f}, AUC={stacking_auc:.4f}, F1={stacking_f1:.4f}\")\n",
        "\n",
        "# Set the best model as our stacking classifier for further analysis\n",
        "final_model = stacking_clf\n",
        "best_model_name = \"Ensemble/ Stacking Classifier\"\n",
        "\n",
        "print(f\"\\n🎯 Selected Model: {best_model_name}\")"
      ],
      "metadata": {
        "id": "Ni4K8HJ1vOBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"⚙️ STACKING MODEL HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define hyperparameter grids for base models\n",
        "base_param_grids = {\n",
        "    'rf__n_estimators': [100, 200],\n",
        "    'rf__max_depth': [10, 20, None],\n",
        "    'rf__min_samples_split': [2, 5],\n",
        "\n",
        "    'svm__C': [0.1, 1, 10],\n",
        "    'svm__gamma': ['scale', 'auto'],\n",
        "    'svm__kernel': ['rbf', 'linear'],\n",
        "\n",
        "    'gb__n_estimators': [100, 200],\n",
        "    'gb__learning_rate': [0.01, 0.1],\n",
        "    'gb__max_depth': [3, 5],\n",
        "\n",
        "    'knn__n_neighbors': [3, 5, 7],\n",
        "    'knn__weights': ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "# Meta-model parameters\n",
        "meta_param_grid = {\n",
        "    'final_estimator__C': [0.1, 1, 10],\n",
        "    'final_estimator__penalty': ['l2']\n",
        "}\n",
        "\n",
        "# Combine parameter grids\n",
        "param_grid = {**base_param_grids, **meta_param_grid}\n",
        "\n",
        "print(\"🔍 Performing Grid Search for Stacking Classifier...\")\n",
        "print(\"Parameter combinations to test:\", len(list(param_grid.keys())))\n",
        "\n",
        "# Create a smaller grid for demonstration (full grid would take too long)\n",
        "simplified_param_grid = {\n",
        "    'rf__n_estimators': [100, 200],\n",
        "    'rf__max_depth': [10, None],\n",
        "    'svm__C': [1, 10],\n",
        "    'gb__n_estimators': [100],\n",
        "    'gb__learning_rate': [0.1],\n",
        "    'final_estimator__C': [1, 10]\n",
        "}\n",
        "\n",
        "# Grid Search with Cross Validation\n",
        "print(\"🚀 Running optimized grid search (this may take a few minutes)...\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    stacking_clf,\n",
        "    simplified_param_grid,\n",
        "    cv=3,  # Reduced CV folds for faster execution\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "best_cv_score = grid_search.best_score_\n",
        "\n",
        "print(f\"\\n🏆 BEST PARAMETERS FOR STACKING CLASSIFIER:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "print(f\"\\n📊 Best Cross-Validation AUC Score: {best_cv_score:.4f}\")\n",
        "\n",
        "# Update final model with best parameters\n",
        "final_model = grid_search.best_estimator_\n",
        "print(f\"\\n✅ Final optimized stacking model ready!\")\n",
        "\n",
        "# Show improvement\n",
        "print(f\"\\n📈 PERFORMANCE IMPROVEMENT:\")\n",
        "print(f\"Before tuning: AUC = {stacking_auc:.4f}\")\n",
        "print(f\"After tuning:  AUC = {best_cv_score:.4f}\")\n",
        "print(f\"Improvement:   {(best_cv_score - stacking_auc)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "-Ik547qfZluq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📊 MODEL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Make predictions with final model\n",
        "y_pred_final = final_model.predict(X_test_scaled)\n",
        "y_pred_proba_final = final_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate all required metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_final)\n",
        "logloss = log_loss(y_test, y_pred_proba_final)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_final)\n",
        "precision = precision_score(y_test, y_pred_final, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_final, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_final, average='weighted')\n",
        "\n",
        "print(\"🎯 FINAL MODEL PERFORMANCE METRICS:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"✅ Accuracy:         {accuracy:.4f}\")\n",
        "print(f\"✅ Logarithmic Loss: {logloss:.4f}\")\n",
        "print(f\"✅ AUC Score:        {auc:.4f}\")\n",
        "print(f\"✅ Precision:        {precision:.4f}\")\n",
        "print(f\"✅ Recall:           {recall:.4f}\")\n",
        "print(f\"✅ F1 Score:         {f1:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "print(f\"\\n📊 CONFUSION MATRIX:\")\n",
        "print(cm)\n",
        "\n",
        "# Visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Confusion Matrix Heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
        "axes[0,0].set_title('Confusion Matrix')\n",
        "axes[0,0].set_xlabel('Predicted')\n",
        "axes[0,0].set_ylabel('Actual')\n",
        "\n",
        "# 2. ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_final)\n",
        "axes[0,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
        "axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[0,1].set_xlim([0.0, 1.0])\n",
        "axes[0,1].set_ylim([0.0, 1.05])\n",
        "axes[0,1].set_xlabel('False Positive Rate')\n",
        "axes[0,1].set_ylabel('True Positive Rate')\n",
        "axes[0,1].set_title('ROC Curve')\n",
        "axes[0,1].legend(loc=\"lower right\")\n",
        "\n",
        "# 3. Feature Importance (if available)\n",
        "if hasattr(final_model, 'feature_importances_'):\n",
        "    feature_importance = final_model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "\n",
        "    # Get top 10 features\n",
        "    top_indices = np.argsort(feature_importance)[-10:]\n",
        "\n",
        "    axes[1,0].barh(range(10), feature_importance[top_indices])\n",
        "    axes[1,0].set_yticks(range(10))\n",
        "    axes[1,0].set_yticklabels([feature_names[i] for i in top_indices])\n",
        "    axes[1,0].set_xlabel('Importance')\n",
        "    axes[1,0].set_title('Top 10 Feature Importance')\n",
        "\n",
        "# 4. Metrics Summary Bar Chart\n",
        "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
        "metrics_values = [accuracy, precision, recall, f1, auc]\n",
        "\n",
        "axes[1,1].bar(metrics_names, metrics_values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])\n",
        "axes[1,1].set_ylim(0, 1)\n",
        "axes[1,1].set_title('Model Performance Metrics')\n",
        "axes[1,1].set_ylabel('Score')\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(f\"\\n📋 DETAILED CLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_test, y_pred_final))"
      ],
      "metadata": {
        "id": "Oc4t4dcncQ7I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}